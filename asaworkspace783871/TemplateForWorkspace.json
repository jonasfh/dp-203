{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "asaworkspace783871"
		},
		"asacosmosdb01_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asacosmosdb01'"
		},
		"asadatalake783871_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'asadatalake783871'"
		},
		"asastore783871_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asastore783871'"
		},
		"asaworkspace783871-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'asaworkspace783871-WorkspaceDefaultSqlServer'"
		},
		"sqlpool01_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlpool01'"
		},
		"sqlpool01_highperf_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlpool01_highperf'"
		},
		"sqlpool01_import01_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlpool01_import01'"
		},
		"sqlpool01_workload01_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlpool01_workload01'"
		},
		"sqlpool01_workload02_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'sqlpool01_workload02'"
		},
		"asadatalake783871_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://asadatalake783871.dfs.core.windows.net"
		},
		"asakeyvault783871_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://asakeyvault783871.vault.azure.net/"
		},
		"asaworkspace783871-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://asadatalake783871.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Copy December Sales')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy Sales",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "TRUNCATE TABLE wwi_perf.Sale_Heap",
								"allowCopyCommand": true,
								"copyCommandSettings": {},
								"disableMetricsCollection": false
							},
							"enableStaging": false,
							"dataIntegrationUnits": 8,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "TransactionId",
											"type": "String"
										},
										"sink": {
											"name": "TransactionId",
											"type": "Guid"
										}
									},
									{
										"source": {
											"name": "CustomerId",
											"type": "Int32"
										},
										"sink": {
											"name": "CustomerId",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "ProductId",
											"type": "Int16"
										},
										"sink": {
											"name": "ProductId",
											"type": "Int16"
										}
									},
									{
										"source": {
											"name": "Quantity",
											"type": "SByte"
										},
										"sink": {
											"name": "Quantity",
											"type": "Byte"
										}
									},
									{
										"source": {
											"name": "Price",
											"type": "Decimal"
										},
										"sink": {
											"name": "Price",
											"type": "Decimal"
										}
									},
									{
										"source": {
											"name": "TotalAmount",
											"type": "Decimal"
										},
										"sink": {
											"name": "TotalAmount",
											"type": "Decimal"
										}
									},
									{
										"source": {
											"name": "TransactionDate",
											"type": "Int32"
										},
										"sink": {
											"name": "TransactionDateId",
											"type": "Int32"
										}
									},
									{
										"source": {
											"name": "ProfitAmount",
											"type": "Decimal"
										},
										"sink": {
											"name": "ProfitAmount",
											"type": "Decimal"
										}
									},
									{
										"source": {
											"name": "Hour",
											"type": "SByte"
										},
										"sink": {
											"name": "Hour",
											"type": "Byte"
										}
									},
									{
										"source": {
											"name": "Minute",
											"type": "SByte"
										},
										"sink": {
											"name": "Minute",
											"type": "Byte"
										}
									},
									{
										"source": {
											"name": "StoreId",
											"type": "Int16"
										},
										"sink": {
											"name": "StoreId",
											"type": "Int16"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "asal400_december_sales",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "asal400_saleheap_asa",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/asal400_december_sales')]",
				"[concat(variables('workspaceId'), '/datasets/asal400_saleheap_asa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://asakeyvault783871.vault.azure.net/secrets/PipelineSecret/944f7dd7714d4ada8e4c4c03058b495f?api-version=7.1",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [
							{
								"activity": "Web1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"variableName": "SecretValue",
							"value": {
								"value": "@activity('Web1').output.value",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"SecretValue": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/User Profiles to Datalake')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "user_profiles_to_datalake",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "user_profiles_to_datalake",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"EcommerceUserProfiles": {},
									"UserProfiles": {},
									"DataLake": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Calculate Top 5 Products",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "user_profiles_to_datalake",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Calculate Top 5 Products",
								"type": "NotebookReference"
							},
							"parameters": {
								"runId": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/user_profiles_to_datalake')]",
				"[concat(variables('workspaceId'), '/notebooks/Calculate Top 5 Products')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Write Campaign Analytics to ASA')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "asal400_lab2_writecampaignanalyticstoasa",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "asal400_lab2_writecampaignanalyticstoasa",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"CampaignAnalytics": {},
									"CampaignAnalyticsASA": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/asal400_lab2_writecampaignanalyticstoasa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Write User Profile Data to ASA')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "write_user_profile_to_asa",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "write_user_profile_to_asa",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"EcommerceUserProfiles": {},
									"UserProfiles": {},
									"UserTopProductPurchasesASA": {},
									"DataLake": {}
								}
							},
							"staging": {
								"linkedService": {
									"referenceName": "asadatalake783871",
									"type": "LinkedServiceReference"
								},
								"folderPath": "staging/userprofiles"
							},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/write_user_profile_to_asa')]",
				"[concat(variables('workspaceId'), '/linkedServices/asadatalake783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_campaign_analytics_source')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asadatalake783871",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "campaignanalytics.csv",
						"folderPath": "campaign-analytics",
						"fileSystem": "wwi-02"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asadatalake783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_customerprofile_cosmosdb')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asacosmosdb01",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "CosmosDbSqlApiCollection",
				"schema": {
					"type": "object",
					"properties": {
						"userId": {
							"type": "integer"
						},
						"cartId": {
							"type": "string"
						},
						"preferredProducts": {
							"type": "array",
							"items": {
								"type": "integer"
							}
						},
						"productReviews": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"productId": {
										"type": "integer"
									},
									"reviewText": {
										"type": "string"
									},
									"reviewDate": {
										"type": "string"
									}
								}
							}
						}
					}
				},
				"typeProperties": {
					"collectionName": "OnlineUserProfile01"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asacosmosdb01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_december_sales')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asadatalake783871",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "sale-20161230-snappy.parquet",
						"folderPath": "campaign-analytics",
						"fileSystem": "wwi-02"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "TransactionId",
						"type": "UTF8"
					},
					{
						"name": "CustomerId",
						"type": "INT32"
					},
					{
						"name": "ProductId",
						"type": "INT_16"
					},
					{
						"name": "Quantity",
						"type": "INT_8"
					},
					{
						"name": "Price",
						"type": "DECIMAL",
						"precision": 38,
						"scale": 18
					},
					{
						"name": "TotalAmount",
						"type": "DECIMAL",
						"precision": 38,
						"scale": 18
					},
					{
						"name": "TransactionDate",
						"type": "INT32"
					},
					{
						"name": "ProfitAmount",
						"type": "DECIMAL",
						"precision": 38,
						"scale": 18
					},
					{
						"name": "Hour",
						"type": "INT_8"
					},
					{
						"name": "Minute",
						"type": "INT_8"
					},
					{
						"name": "StoreId",
						"type": "INT_16"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asadatalake783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_ecommerce_userprofiles_source')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "asadatalake783871",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "online-user-profiles-02",
						"fileSystem": "wwi-02"
					}
				},
				"schema": {
					"type": "object",
					"properties": {
						"visitorId": {
							"type": "integer"
						},
						"topProductPurchases": {
							"type": "array",
							"items": {
								"type": "object",
								"properties": {
									"productId": {
										"type": "integer"
									},
									"itemsPurchasedLast12Months": {
										"type": "integer"
									}
								}
							}
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asadatalake783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_saleheap_asa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sqlpool01_import01",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "TransactionId",
						"type": "uniqueidentifier"
					},
					{
						"name": "CustomerId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProductId",
						"type": "smallint",
						"precision": 5
					},
					{
						"name": "Quantity",
						"type": "tinyint",
						"precision": 3
					},
					{
						"name": "Price",
						"type": "decimal",
						"precision": 9,
						"scale": 2
					},
					{
						"name": "TotalAmount",
						"type": "decimal",
						"precision": 9,
						"scale": 2
					},
					{
						"name": "TransactionDateId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProfitAmount",
						"type": "decimal",
						"precision": 9,
						"scale": 2
					},
					{
						"name": "Hour",
						"type": "tinyint",
						"precision": 3
					},
					{
						"name": "Minute",
						"type": "tinyint",
						"precision": 3
					},
					{
						"name": "StoreId",
						"type": "smallint",
						"precision": 5
					}
				],
				"typeProperties": {
					"schema": "wwi_perf",
					"table": "Sale_Heap"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sqlpool01_import01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_wwi_campaign_analytics_asa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sqlpool01",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "Region",
						"type": "nvarchar"
					},
					{
						"name": "Country",
						"type": "nvarchar"
					},
					{
						"name": "ProductCategory",
						"type": "nvarchar"
					},
					{
						"name": "CampaignName",
						"type": "nvarchar"
					},
					{
						"name": "Revenue",
						"type": "decimal",
						"precision": 10,
						"scale": 2
					},
					{
						"name": "RevenueTarget",
						"type": "decimal",
						"precision": 10,
						"scale": 2
					},
					{
						"name": "City",
						"type": "nvarchar"
					},
					{
						"name": "State",
						"type": "nvarchar"
					}
				],
				"typeProperties": {
					"schema": "wwi",
					"table": "CampaignAnalytics"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sqlpool01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_wwi_usertopproductpurchases_asa')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sqlpool01",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "UserId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProductId",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ItemsPurchasedLast12Months",
						"type": "int",
						"precision": 10
					},
					{
						"name": "IsTopProduct",
						"type": "bit"
					},
					{
						"name": "IsPreferredProduct",
						"type": "bit"
					}
				],
				"typeProperties": {
					"schema": "wwi",
					"table": "UserTopProductPurchases"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/sqlpool01')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asacosmosdb01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('asacosmosdb01_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asadatalake783871')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asadatalake783871_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('asadatalake783871_accountKey')]"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asakeyvault783871')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('asakeyvault783871_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asastore783871')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('asastore783871_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asaworkspace783871-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('asaworkspace783871-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asaworkspace783871-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('asaworkspace783871-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlpool01_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "asakeyvault783871",
							"type": "LinkedServiceReference"
						},
						"secretName": "SQL-USER-ASA"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asakeyvault783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool01_highperf')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlpool01_highperf_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "asakeyvault783871",
							"type": "LinkedServiceReference"
						},
						"secretName": "SQL-USER-ASA"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asakeyvault783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool01_import01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlpool01_import01_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "asakeyvault783871",
							"type": "LinkedServiceReference"
						},
						"secretName": "SQL-USER-ASA"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asakeyvault783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool01_workload01')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlpool01_workload01_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "asakeyvault783871",
							"type": "LinkedServiceReference"
						},
						"secretName": "SQL-USER-ASA"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asakeyvault783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpool01_workload02')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('sqlpool01_workload02_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "asakeyvault783871",
							"type": "LinkedServiceReference"
						},
						"secretName": "SQL-USER-ASA"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/asakeyvault783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureIntegrationRuntime01')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "MemoryOptimized",
							"coreCount": 16,
							"timeToLive": 60
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/asal400_lab2_writecampaignanalyticstoasa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "asal400_campaign_analytics_source",
								"type": "DatasetReference"
							},
							"name": "CampaignAnalytics"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "asal400_wwi_campaign_analytics_asa",
								"type": "DatasetReference"
							},
							"name": "CampaignAnalyticsASA"
						}
					],
					"transformations": [
						{
							"name": "MapCampaignAnalytics"
						},
						{
							"name": "ConvertColumnTypesAndValues"
						},
						{
							"name": "SelectCampaignAnalyticsColumns"
						}
					],
					"scriptLines": [
						"source(output(",
						"          {_col0_} as string,",
						"          {_col1_} as string,",
						"          {_col2_} as string,",
						"          {_col3_} as string,",
						"          {_col4_} as string,",
						"          {_col5_} as double,",
						"          {_col6_} as string,",
						"          {_col7_} as double,",
						"          {_col8_} as string,",
						"          {_col9_} as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     skipLines: 1) ~> CampaignAnalytics",
						"CampaignAnalytics select(mapColumn(",
						"          Region = {_col0_},",
						"          Country = {_col1_},",
						"          ProductCategory = {_col2_},",
						"          CampaignName = {_col3_},",
						"          RevenuePart1 = {_col4_},",
						"          Revenue = {_col5_},",
						"          RevenueTargetPart1 = {_col6_},",
						"          RevenueTarget = {_col7_},",
						"          City = {_col8_},",
						"          State = {_col9_}",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> MapCampaignAnalytics",
						"MapCampaignAnalytics derive(Revenue = toDecimal(replace(concat(toString(RevenuePart1), toString(Revenue)), '\\\\', ''), 10, 2, '$###,###.##'),",
						"          RevenueTarget = toDecimal(replace(concat(toString(RevenueTargetPart1), toString(RevenueTarget)), '\\\\', ''), 10, 2, '$###,###.##')) ~> ConvertColumnTypesAndValues",
						"ConvertColumnTypesAndValues select(mapColumn(",
						"          Region,",
						"          Country,",
						"          ProductCategory,",
						"          CampaignName,",
						"          Revenue,",
						"          RevenueTarget,",
						"          City,",
						"          State",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> SelectCampaignAnalyticsColumns",
						"SelectCampaignAnalyticsColumns sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Region as string,",
						"          Country as string,",
						"          ProductCategory as string,",
						"          CampaignName as string,",
						"          Revenue as decimal(10,2),",
						"          RevenueTarget as decimal(10,2),",
						"          City as string,",
						"          State as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     truncate:true,",
						"     format: 'table',",
						"     staged: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> CampaignAnalyticsASA"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/asal400_campaign_analytics_source')]",
				"[concat(variables('workspaceId'), '/datasets/asal400_wwi_campaign_analytics_asa')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/user_profiles_to_datalake')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "asal400_ecommerce_userprofiles_source",
								"type": "DatasetReference"
							},
							"name": "EcommerceUserProfiles"
						},
						{
							"dataset": {
								"referenceName": "asal400_customerprofile_cosmosdb",
								"type": "DatasetReference"
							},
							"name": "UserProfiles"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "asadatalake783871",
								"type": "LinkedServiceReference"
							},
							"name": "DataLake"
						}
					],
					"transformations": [
						{
							"name": "userId"
						},
						{
							"name": "UserTopProducts"
						},
						{
							"name": "DerivedProductColumns"
						},
						{
							"name": "UserPreferredProducts"
						},
						{
							"name": "JoinTopProductsWithPreferredProducts"
						},
						{
							"name": "DerivedColumnsForMerge"
						},
						{
							"name": "Filter1"
						}
					],
					"script": "source(output(\n\t\tvisitorId as string,\n\t\ttopProductPurchases as (productId as string, itemsPurchasedLast12Months as string)[]\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tdocumentForm: 'arrayOfDocuments',\n\twildcardPaths:['online-user-profiles-02/*.json']) ~> EcommerceUserProfiles\nsource(output(\n\t\tcartId as string,\n\t\tpreferredProducts as integer[],\n\t\tproductReviews as (productId as integer, reviewDate as string, reviewText as string)[],\n\t\tuserId as integer\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'document') ~> UserProfiles\nEcommerceUserProfiles derive(visitorId = toInteger(visitorId)) ~> userId\nuserId foldDown(unroll(topProductPurchases),\n\tmapColumn(\n\t\tvisitorId,\n\t\tproductId = topProductPurchases.productId,\n\t\titemsPurchasedLast12Months = topProductPurchases.itemsPurchasedLast12Months\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> UserTopProducts\nUserTopProducts derive(productId = toInteger(productId),\n\t\titemsPurchasedLast12Months = toInteger(itemsPurchasedLast12Months)) ~> DerivedProductColumns\nUserProfiles foldDown(unroll(preferredProducts),\n\tmapColumn(\n\t\tpreferredProductId = preferredProducts,\n\t\tuserId\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> UserPreferredProducts\nDerivedProductColumns, UserPreferredProducts join(visitorId == userId,\n\tjoinType:'outer',\n\tmatchType:'exact',\n\tignoreSpaces: false,\n\tpartitionBy('hash', 30,\n\t\tproductId\n\t),\n\tbroadcast: 'left')~> JoinTopProductsWithPreferredProducts\nJoinTopProductsWithPreferredProducts derive(isTopProduct = toBoolean(iif(isNull(productId), 'false', 'true')),\n\t\tisPreferredProduct = toBoolean(iif(isNull(preferredProductId), 'false', 'true')),\n\t\tproductId = iif(isNull(productId), preferredProductId, productId),\n\t\tuserId = iif(isNull(userId), visitorId, userId)) ~> DerivedColumnsForMerge\nDerivedColumnsForMerge filter(!isNull(productId)) ~> Filter1\nFilter1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tformat: 'delta',\n\tcompressionType: 'snappy',\n\tcompressionLevel: 'Fastest',\n\tfileSystem: 'wwi-02',\n\tfolderPath: 'top-products',\n\ttruncate:true,\n\tmergeSchema: false,\n\tautoCompact: false,\n\toptimizedWrite: false,\n\tvacuum: 0,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tumask: 0022,\n\tpreCommands: [],\n\tpostCommands: [],\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\tmapColumn(\n\t\tvisitorId,\n\t\tproductId,\n\t\titemsPurchasedLast12Months,\n\t\tpreferredProductId,\n\t\tuserId,\n\t\tisTopProduct,\n\t\tisPreferredProduct\n\t)) ~> DataLake"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/asal400_ecommerce_userprofiles_source')]",
				"[concat(variables('workspaceId'), '/datasets/asal400_customerprofile_cosmosdb')]",
				"[concat(variables('workspaceId'), '/linkedServices/asadatalake783871')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/write_user_profile_to_asa')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "asal400_ecommerce_userprofiles_source",
								"type": "DatasetReference"
							},
							"name": "EcommerceUserProfiles"
						},
						{
							"dataset": {
								"referenceName": "asal400_customerprofile_cosmosdb",
								"type": "DatasetReference"
							},
							"name": "UserProfiles"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "asal400_wwi_usertopproductpurchases_asa",
								"type": "DatasetReference"
							},
							"name": "UserTopProductPurchasesASA"
						},
						{
							"linkedService": {
								"referenceName": "asaworkspace783871-WorkspaceDefaultStorage",
								"type": "LinkedServiceReference"
							},
							"name": "DataLake"
						}
					],
					"transformations": [
						{
							"name": "userId"
						},
						{
							"name": "UserTopProducts"
						},
						{
							"name": "DeriveProductColumns"
						},
						{
							"name": "UserPreferredProducts"
						},
						{
							"name": "JoinTopProductsWithPreferredProducts"
						},
						{
							"name": "DerivedColumnsForMerge"
						},
						{
							"name": "FilterEmptyProducts"
						}
					],
					"scriptLines": [
						"source(output(",
						"          visitorId as integer,",
						"          topProductPurchases as (productId as integer, itemsPurchasedLast12Months as integer)[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments',",
						"     wildcardPaths:['online-user-profiles-02/*.json']) ~> EcommerceUserProfiles",
						"source(output(",
						"          userId as integer,",
						"          cartId as string,",
						"          preferredProducts as integer[],",
						"          productReviews as (productId as integer, reviewDate as string, reviewText as string)[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'document') ~> UserProfiles",
						"EcommerceUserProfiles derive(visitorId = toInteger(visitorId)) ~> userId",
						"userId foldDown(unroll(topProductPurchases),",
						"     mapColumn(",
						"          visitorId,",
						"          productId = topProductPurchases.productId,",
						"          itemsPurchasedLast12Months = topProductPurchases.itemsPurchasedLast12Months",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> UserTopProducts",
						"UserTopProducts derive(productId = toInteger(productId),",
						"          itemsPurchasedLast12Months = toInteger(itemsPurchasedLast12Months)) ~> DeriveProductColumns",
						"UserProfiles foldDown(unroll(preferredProducts),",
						"     mapColumn(",
						"          userId,",
						"          preferredProductId = preferredProducts",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> UserPreferredProducts",
						"DeriveProductColumns, UserPreferredProducts join(visitorId == userId,",
						"     joinType:'outer',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     partitionBy('hash', 30,",
						"          productId",
						"     ),",
						"     broadcast: 'left')~> JoinTopProductsWithPreferredProducts",
						"JoinTopProductsWithPreferredProducts derive(isTopProduct = toBoolean(iif(isNull(productId), 'false', 'true')),",
						"          isPreferredProduct = toBoolean(iif(isNull(preferredProductId), 'false', 'true')),",
						"          productId = iif(isNull(productId), preferredProductId, productId),",
						"          userId = iif(isNull(userId), visitorId, userId)) ~> DerivedColumnsForMerge",
						"DerivedColumnsForMerge filter(!isNull(productId)) ~> FilterEmptyProducts",
						"FilterEmptyProducts sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          UserId as integer,",
						"          ProductId as integer,",
						"          ItemsPurchasedLast12Months as integer,",
						"          IsTopProduct as boolean,",
						"          IsPreferredProduct as boolean",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     truncate:true,",
						"     format: 'table',",
						"     staged: true,",
						"     allowCopyCommand: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          UserId = userId,",
						"          ProductId = productId,",
						"          ItemsPurchasedLast12Months = itemsPurchasedLast12Months,",
						"          IsTopProduct = isTopProduct,",
						"          IsPreferredProduct = isPreferredProduct",
						"     )) ~> UserTopProductPurchasesASA",
						"FilterEmptyProducts sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'delta',",
						"     compressionType: 'snappy',",
						"     compressionLevel: 'Fastest',",
						"     fileSystem: 'wwi-02',",
						"     folderPath: 'top-products',",
						"     truncate:true,",
						"     mergeSchema: false,",
						"     autoCompact: false,",
						"     optimizedWrite: false,",
						"     vacuum: 0,",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          visitorId,",
						"          productId,",
						"          itemsPurchasedLast12Months,",
						"          userId,",
						"          preferredProductId,",
						"          isTopProduct,",
						"          isPreferredProduct",
						"     )) ~> DataLake"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/asal400_ecommerce_userprofiles_source')]",
				"[concat(variables('workspaceId'), '/datasets/asal400_customerprofile_cosmosdb')]",
				"[concat(variables('workspaceId'), '/datasets/asal400_wwi_usertopproductpurchases_asa')]",
				"[concat(variables('workspaceId'), '/linkedServices/asaworkspace783871-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Column Level Security')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "    /*  Column-level security feature in Azure Synapse simplifies the design and coding of security in application.\n        It ensures column level security by restricting column access to protect sensitive data. */\n\n    /* Scenario: In this scenario we will be working with two users. The first one is the CEO, he has access to all\n        data. The second one is DataAnalystMiami, this user doesn't have access to the confidential Revenue column\n        in the Sales table. Follow this lab, one step at a time to see how Column-level security removes access to the\n        Revenue column to DataAnalystMiami */\n\n    --Step 1: Let us see how this feature in Azure Synapse works. Before that let us have a look at the Campaign table.\n    select  Top 100 * from wwi_Security.Sale\n    where City is not null and state is not null\n\n    /*  Consider a scenario where there are two users.\n        A CEO, who is an authorized  personnel with access to all the information in the database\n        and a Data Analyst, to whom only required information should be presented.*/\n\n    -- Step:2 Verify the existence of the 'CEO' and 'DataAnalystMiami' users in the Datawarehouse.\n    SELECT Name as [User1] FROM sys.sysusers WHERE name = N'CEO';\n    SELECT Name as [User2] FROM sys.sysusers WHERE name = N'DataAnalystMiami';\n\n\n    -- Step:3 Now let us enforce column level security for the DataAnalystMiami.\n    /*  The Sales table in the warehouse has information like ProductID, Analyst, Product, CampaignName, Quantity, Region, State, City, RevenueTarget and Revenue.\n        The Revenue generated from every campaign is classified and should be hidden from DataAnalystMiami.\n    */\n\n    REVOKE SELECT ON wwi_security.Sale FROM DataAnalystMiami;\n    GRANT SELECT ON wwi_security.Sale([ProductID], [Analyst], [Product], [CampaignName],[Quantity], [Region], [State], [City], [RevenueTarget]) TO DataAnalystMiami;\n    -- This provides DataAnalystMiami access to all the columns of the Sale table but Revenue.\n\n    -- Step:4 Then, to check if the security has been enforced, we execute the following query with current User As 'DataAnalystMiami', this will result in an error\n    --  since DataAnalystMiami doesn't have select access to the Revenue column\n    EXECUTE AS USER ='DataAnalystMiami';\n    select TOP 100 * from wwi_security.Sale;\n    ---\n    -- The following query will succeed since we are not including the Revenue column in the query.\n    EXECUTE AS USER ='DataAnalystMiami';\n    select [ProductID], [Analyst], [Product], [CampaignName],[Quantity], [Region], [State], [City], [RevenueTarget] from wwi_security.Sale;\n    \n    -- Step:5 Whereas, the CEO of the company should be authorized with all the information present in the warehouse.To do so, we execute the following query.\n    Revert;\n    GRANT SELECT ON wwi_security.Sale TO CEO;  --Full access to all columns.\n\n    -- Step:6 Let us check if our CEO user can see all the information that is present. Assign Current User As 'CEO' and the execute the query\n    EXECUTE AS USER ='CEO'\n    select * from wwi_security.Sale\n    Revert;\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Warehouse Optimization')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\r\n    FS.CustomerID\r\n    ,MIN(FS.Quantity) as MinQuantity\r\n    ,MAX(FS.Quantity) as MaxQuantity\r\n    ,AVG(FS.Price) as AvgPrice\r\n    ,AVG(FS.TotalAmount) as AvgTotalAmount\r\n    ,AVG(FS.ProfitAmount) as AvgProfitAmount\r\n    ,COUNT(DISTINCT FS.StoreId) as DistinctStores\r\nFROM\r\n    wwi_perf.Sale_Heap FS\r\nGROUP BY\r\n    FS.CustomerId\r\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLPool01",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dynamic Data Masking')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "    -------------------------------------------------------------------------Dynamic Data Masking (DDM)----------------------------------------------------------------------------------------------------------\r\n    /*  Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers\r\n        to designate how much of the sensitive data to reveal with minimal impact on the application layer.\r\n        Let see how */\r\n\r\n    /* Scenario: WWI has identified sensitive information in the CustomerInfo table. They would like us to \r\n        obfuscate the CreditCard and Email columns of the CustomerInfo table to DataAnalysts */\r\n\r\n    -- Step:1 Let us first get a view of CustomerInfo table.\r\n    SELECT TOP (100) * FROM wwi_security.CustomerInfo;\r\n\r\n    -- Step:2 Let's confirm that there are no Dynamic Data Masking (DDM) applied on columns.\r\n    SELECT c.name, tbl.name as table_name, c.is_masked, c.masking_function  \r\n    FROM sys.masked_columns AS c  \r\n    JOIN sys.tables AS tbl\r\n        ON c.[object_id] = tbl.[object_id]  \r\n    WHERE is_masked = 1\r\n        AND tbl.name = 'CustomerInfo';\r\n    -- No results returned verify that no data masking has been done yet.\r\n\r\n    -- Step:3 Now lets mask 'CreditCard' and 'Email' Column of 'CustomerInfo' table.\r\n    ALTER TABLE wwi_security.CustomerInfo  \r\n    ALTER COLUMN [CreditCard] ADD MASKED WITH (FUNCTION = 'partial(0,\"XXXX-XXXX-XXXX-\",4)');\r\n    GO\r\n    ALTER TABLE wwi_security.CustomerInfo\r\n    ALTER COLUMN Email ADD MASKED WITH (FUNCTION = 'email()');\r\n    GO\r\n    -- The columns are sucessfully masked.\r\n\r\n    -- Step:4 Let's see Dynamic Data Masking (DDM) applied on the two columns.\r\n    SELECT c.name, tbl.name as table_name, c.is_masked, c.masking_function  \r\n    FROM sys.masked_columns AS c  \r\n    JOIN sys.tables AS tbl\r\n        ON c.[object_id] = tbl.[object_id]  \r\n    WHERE is_masked = 1\r\n        AND tbl.name ='CustomerInfo';\r\n\r\n    -- Step:5 Now, let us grant SELECT permission to 'DataAnalystMiami' on the 'CustomerInfo' table.\r\n   GRANT SELECT ON wwi_security.CustomerInfo TO DataAnalystMiami;  \r\n\r\n    -- Step:6 Logged in as  'DataAnalystMiami' let us execute the select query and view the result.\r\n    EXECUTE AS USER = 'DataAnalystMiami';  \r\n    SELECT * FROM wwi_security.CustomerInfo;\r\n\r\n    -- Step:7 Let us remove the data masking using UNMASK permission\r\n    GRANT UNMASK TO DataAnalystMiami;\r\n    EXECUTE AS USER = 'DataAnalystMiami';  \r\n    SELECT *\r\n    FROM wwi_security.CustomerInfo;\r\n    revert;\r\n    REVOKE UNMASK TO DataAnalystMiami;  \r\n\r\n    ----step:8 Reverting all the changes back to as it was.\r\n    ALTER TABLE wwi_security.CustomerInfo\r\n    ALTER COLUMN CreditCard DROP MASKED;\r\n    GO\r\n    ALTER TABLE wwi_security.CustomerInfo\r\n    ALTER COLUMN Email DROP MASKED;\r\n    GO\r\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"name": "SQLPool01",
						"type": "SqlPool"
					}
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LAB 5 oppgave 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM sys.schemas WHERE name = 'wwi_staging';\n\n\n-- Create saleheap table\nCREATE TABLE [wwi_staging].[SaleHeap]\n( \n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    HEAP\n);\n\n\n-- Create sale table in staging area\n\nCREATE TABLE [wwi_staging].[Sale]\n(\n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ( [CustomerId] ),\n    CLUSTERED COLUMNSTORE INDEX,\n    PARTITION\n    (\n        [TransactionDate] RANGE RIGHT FOR VALUES (20100101, 20100201, 20100301, 20100401, 20100501, 20100601, 20100701, 20100801, 20100901, 20101001, 20101101, 20101201, 20110101, 20110201, 20110301, 20110401, 20110501, 20110601, 20110701, 20110801, 20110901, 20111001, 20111101, 20111201, 20120101, 20120201, 20120301, 20120401, 20120501, 20120601, 20120701, 20120801, 20120901, 20121001, 20121101, 20121201, 20130101, 20130201, 20130301, 20130401, 20130501, 20130601, 20130701, 20130801, 20130901, 20131001, 20131101, 20131201, 20140101, 20140201, 20140301, 20140401, 20140501, 20140601, 20140701, 20140801, 20140901, 20141001, 20141101, 20141201, 20150101, 20150201, 20150301, 20150401, 20150501, 20150601, 20150701, 20150801, 20150901, 20151001, 20151101, 20151201, 20160101, 20160201, 20160301, 20160401, 20160501, 20160601, 20160701, 20160801, 20160901, 20161001, 20161101, 20161201, 20170101, 20170201, 20170301, 20170401, 20170501, 20170601, 20170701, 20170801, 20170901, 20171001, 20171101, 20171201, 20180101, 20180201, 20180301, 20180401, 20180501, 20180601, 20180701, 20180801, 20180901, 20181001, 20181101, 20181201, 20190101, 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201)\n    )\n)\n\n\n-- Start copying using polybase\nCREATE EXTERNAL DATA SOURCE ABSS\nWITH\n( TYPE = HADOOP,\n    LOCATION = 'abfss://wwi-02@asadatalake783871.dfs.core.windows.net'\n);\n\n\n-- Create the external resources\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\nCREATE SCHEMA [wwi_external];\nGO\n-- External tables dont support uniqueidentifier columns\nCREATE EXTERNAL TABLE [wwi_external].Sales\n    (\n        [TransactionId] [nvarchar](36)  NOT NULL,\n        [CustomerId] [int]  NOT NULL,\n        [ProductId] [smallint]  NOT NULL,\n        [Quantity] [smallint]  NOT NULL,\n        [Price] [decimal](9,2)  NOT NULL,\n        [TotalAmount] [decimal](9,2)  NOT NULL,\n        [TransactionDate] [int]  NOT NULL,\n        [ProfitAmount] [decimal](9,2)  NOT NULL,\n        [Hour] [tinyint]  NOT NULL,\n        [Minute] [tinyint]  NOT NULL,\n        [StoreId] [smallint]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/sale-small/Year=2019',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = [ParquetFormat]  \n    )  \n;\n\n\n-- Inserting data in staging tables\n\nINSERT INTO [wwi_staging].[SaleHeap]\nSELECT *\nFROM [wwi_external].[Sales];\n\n-- Vis hva som finnes nå\n-- Skal finnes 4124857\nSELECT COUNT(1) FROM wwi_staging.SaleHeap(nolock);\n\n-- Slett data, skal bruke COPY istedenfor polybase\nTRUNCATE TABLE wwi_staging.SaleHeap;\nGO\n\n-- Bruker COPY - funksjonalitet. Trenger ikke staging for denne funksjonaliteten\nCOPY INTO wwi_staging.SaleHeap\nFROM 'https://asadatalake783871.dfs.core.windows.net/wwi-02/sale-small/Year=2019'\nWITH (\n    FILE_TYPE = 'PARQUET',\n    COMPRESSION = 'SNAPPY'\n)\nGO\n\n-- Sjekk resultat\n-- Skal finnes 4124857\nSELECT COUNT(1) FROM wwi_staging.SaleHeap(nolock);\n\n/* \nBruk COPY for ikke-standard tekst-format: \n\nTextformat: \n20200421.114892.130282.159488.172105.196533,20200420.109934.108377.122039.101946.100712,20200419.253714.357583.452690.553447.653921\n*/\n\nCREATE TABLE [wwi_staging].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nGO\n\nCOPY INTO wwi_staging.DailySalesCounts\nFROM 'https://asadatalake783871.dfs.core.windows.net/wwi-02/campaign-analytics/dailycounts.txt'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIELDTERMINATOR='.',\n    ROWTERMINATOR=','\n)\nGO\n\n-- Sjekk resultatet\nSELECT * FROM [wwi_staging].DailySalesCounts\nORDER BY [Date] DESC;\n\n/*\nForsøk på bruk av polibase for å laste tekstfil med ikke-standard delimiters\n\nFEILER: \n*/\n/*\nCREATE EXTERNAL FILE FORMAT csv_dailysales\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = '.',\n        DATE_FORMAT = '',\n        USE_TYPE_DEFAULT = False\n    )\n);\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/campaign-analytics/dailycounts.txt',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = csv_dailysales\n    )  \nGO\n\nINSERT INTO [wwi_staging].[DailySalesCounts]\nSELECT *\nFROM [wwi_external].[DailySalesCounts];\n*/\n\n/*\nSpørringene ovenfor feiler: \n    HdfsBridge::recordReaderFillBuffer - Unexpected error encountered filling record \n    reader buffer: HadoopExecutionException: Too many columns in the line.\n\nHADOOP støtter kun \\n, \\r eller \\n\\r som line - terminators\n*/\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 5 oppgave 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Exercise 2 - Petabyte-scale ingestion with Azure Synapse Pipelines",
				"content": {
					"query": "/*\nHer skal man la en bruker få økte ressurser for ån kjøre en større import-jobb\n*/\n\n\n-- Drop objects if they exist\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE [name] = 'HeavyLoader')\nBEGIN\n    DROP WORKLOAD CLASSIFIER HeavyLoader\nEND;\nIF EXISTS (SELECT * FROM sys.workload_management_workload_groups WHERE name = 'BigDataLoad')\nBEGIN\n    DROP WORKLOAD GROUP BigDataLoad\nEND;\n\n--Create workload group\nCREATE WORKLOAD GROUP BigDataLoad WITH\n  (\n      MIN_PERCENTAGE_RESOURCE = 50, -- integer value\n      REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25, --  (guaranteed min 4 concurrency)\n      CAP_PERCENTAGE_RESOURCE = 100\n  );\n\n-- Create workload classifier\nCREATE WORKLOAD Classifier HeavyLoader WITH\n(\n    Workload_Group ='BigDataLoad',\n    MemberName='asa.sql.import01',\n    IMPORTANCE = HIGH\n);\n\n-- Sjekk at du finner HeavyLoader - classifier\nSELECT * FROM sys.workload_management_workload_classifiers;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 6 oppg 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nLag tabeller: \n*/\n\nCREATE TABLE [wwi].[UserTopProductPurchases]\n(\n    [UserId] [int]  NOT NULL,\n    [ProductId] [int]  NOT NULL,\n    [ItemsPurchasedLast12Months] [int]  NULL,\n    [IsTopProduct] [bit]  NOT NULL,\n    [IsPreferredProduct] [bit]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ( [UserId] ),\n    CLUSTERED COLUMNSTORE INDEX\n); \n\nCREATE TABLE [wwi].[CampaignAnalytics]\n(\n    [Region] [nvarchar](50)  NOT NULL,\n    [Country] [nvarchar](30)  NOT NULL,\n    [ProductCategory] [nvarchar](50)  NOT NULL,\n    [CampaignName] [nvarchar](500)  NOT NULL,\n    [Revenue] [decimal](10,2)  NULL,\n    [RevenueTarget] [decimal](10,2)  NULL,\n    [City] [nvarchar](50)  NULL,\n    [State] [nvarchar](25)  NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ( [Region] ),\n    CLUSTERED COLUMNSTORE INDEX\n); \n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 9 - oppg 3 - User Profile HTAP')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE master\nGO\n\nIF DB_ID (N'Profiles') IS NULL\nBEGIN\n    CREATE DATABASE Profiles;\nEND\nGO\n\nUSE Profiles\nGO\n\nDROP VIEW IF EXISTS OnlineUserProfile01;\nGO\n\nCREATE VIEW OnlineUserProfile01\nAS\nSELECT\n    *\nFROM OPENROWSET(\n    'CosmosDB',\n    N'account=asacosmosdb783871;database=CustomerProfile;key=UQNP7J4fDur04uMUPNhOgtpNDZOb1YAlxYOgG0OmvEutI4vtSgSeqlUQSHFhrEINijmJPo18n7wLACDbVETsKQ==',\n    OnlineUserProfile01\n)\nWITH (\n    userId bigint,\n    cartId varchar(50),\n    preferredProducts varchar(max),\n    productReviews varchar(max)\n) AS profiles\nCROSS APPLY OPENJSON (productReviews)\nWITH (\n    productId bigint,\n    reviewText varchar(1000)\n) AS reviews\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Profiles",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Row Level Security')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\tRow level Security (RLS) in Azure Synapse enables us to use group membership to control access to rows in a table.\n\tAzure Synapse applies the access restriction every time the data access is attempted from any user. \n\tLet see how we can implement row level security in Azure Synapse.*/\n\n----------------------------------Row-Level Security (RLS), 1: Filter predicates------------------------------------------------------------------\n-- Step:1 The Sale table has two Analyst values: DataAnalystMiami and DataAnalystSanDiego. \n--     Each analyst has jurisdiction across a specific Region. DataAnalystMiami on the South East Region\n--      and DataAnalystSanDiego on the Far West region.\nSELECT DISTINCT Analyst, Region FROM wwi_security.Sale order by Analyst ;\n\n/* Scenario: WWI requires that an Analyst only see the data for their own data from their own region. The CEO should see ALL data.\n    In the Sale table, there is an Analyst column that we can use to filter data to a specific Analyst value. */\n\n/* We will define this filter using what is called a Security Predicate. This is an inline table-valued function that allows\n    us to evaluate additional logic, in this case determining if the Analyst executing the query is the same as the Analyst\n    specified in the Analyst column in the row. The function returns 1 (will return the row) when a row in the Analyst column is the same as the \n    user executing the query (@Analyst = USER_NAME()) or if the user executing the query is the CEO user (USER_NAME() = 'CEO')\n    whom has access to all data.\n*/\n\n-- Review any existing security predicates in the database\nSELECT * FROM sys.security_predicates\n\n--Step:2 Create a new Schema to hold the security predicate, then define the predicate function. It returns 1 (or True) when\n--  a row should be returned in the parent query.\nGO\n\nCREATE FUNCTION wwi_security.fn_securitypredicate(@Analyst AS sysname)  \n    RETURNS TABLE  \nWITH SCHEMABINDING  \nAS  \n    RETURN SELECT 1 AS fn_securitypredicate_result\n    WHERE @Analyst = USER_NAME() OR USER_NAME() = 'CEO'\nGO\n-- Now we define security policy that adds the filter predicate to the Sale table. This will filter rows based on their login name.\nCREATE SECURITY POLICY SalesFilter  \nADD FILTER PREDICATE wwi_security.fn_securitypredicate(Analyst)\nON wwi_security.Sale\nWITH (STATE = ON);\n\n------ Allow SELECT permissions to the Sale Table.------\nGRANT SELECT ON wwi_security.Sale TO CEO, DataAnalystMiami, DataAnalystSanDiego;\n\n-- Step:3 Let us now test the filtering predicate, by selecting data from the Sale table as 'DataAnalystMiami' user.\nEXECUTE AS USER = 'DataAnalystMiami' \nSELECT * FROM wwi_security.Sale;\n\nrevert;\n-- As we can see, the query has returned rows here Login name is DataAnalystMiami\n\n-- Step:4 Let us test the same for  'DataAnalystSanDiego' user.\nEXECUTE AS USER = 'DataAnalystSanDiego';\nSELECT * FROM wwi_security.Sale;\nrevert;\n-- RLS is working indeed.\n\n-- Step:5 The CEO should be able to see all rows in the table.\nEXECUTE AS USER = 'CEO';  \nSELECT * FROM wwi_security.Sale;\nrevert;\n-- And he can.\n\n--Step:6 To disable the security policy we just created above, we execute the following.\nALTER SECURITY POLICY SalesFilter  \nWITH (STATE = OFF);\n\nDROP SECURITY POLICY SalesFilter;\nDROP FUNCTION wwi_security.fn_securitypredicate;\n\n\n\n/*\n    Testing to implement row level security as a view instead of the above method\n*/\n\nREVERT GO\nDROP VIEW IF EXISTS wwi_security.sale_user_view GO\nCREATE VIEW wwi_security.sale_user_view as select * from wwi_security.sale where Analyst=USER_NAME() or USER_NAME()='CEO' GO\nGRANT SELECT ON wwi_security.sale_user_view TO CEO, DataAnalystMiami, DataAnalystSanDiego GO\n\nEXECUTE AS USER = 'DataAnalystSanDiego' \nSELECT top 100  * FROM wwi_security.sale_user_view;\nEXECUTE AS USER = 'DataAnalystMiami' \nSELECT top 100  * FROM wwi_security.sale_user_view;\nEXECUTE AS USER = 'CEO' \nSELECT top 100  * FROM wwi_security.sale_user_view;\n\n-- This will not actually implement security measures, only add filtering to the view. The user could possibly get access\n-- to the data through other means...\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Test lab 6 oppg 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT ProductCategory\n,SUM(Revenue) AS TotalRevenue\n,SUM(RevenueTarget) AS TotalRevenueTarget\n,(SUM(RevenueTarget) - SUM(Revenue)) AS Delta\nFROM [wwi].[CampaignAnalytics]\nGROUP BY ProductCategory",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SQLPool01",
						"poolName": "SQLPool01"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Calculate Top 5 Products')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b0fb5cea-7f1c-41e5-acb8-05d41b152b1b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e1b7229d-48f4-49dd-a500-471738fa34b2/resourceGroups/data-engineering-synapse-783871/providers/Microsoft.Synapse/workspaces/asaworkspace783871/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://asaworkspace783871.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://wwi-02@asadatalake783871.dfs.core.windows.net/top-products/*.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topPurchases = df.select(\r\n",
							"    \"UserId\", \"ProductId\",\r\n",
							"    \"ItemsPurchasedLast12Months\", \"IsTopProduct\",\r\n",
							"    \"IsPreferredProduct\")\r\n",
							"\r\n",
							"# Populate a temporary view so we can query from SQL\r\n",
							"topPurchases.createOrReplaceTempView(\"top_purchases\")\r\n",
							"\r\n",
							"topPurchases.show(100)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"topPreferredProducts = (topPurchases\r\n",
							"    .filter( col(\"IsTopProduct\") == True)\r\n",
							"    .filter( col(\"IsPreferredProduct\") == True)\r\n",
							"    .orderBy( col(\"ItemsPurchasedLast12Months\").desc() ))\r\n",
							"\r\n",
							"topPreferredProducts.show(100)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE OR REPLACE TEMPORARY VIEW top_5_products\r\n",
							"AS\r\n",
							"    select UserId, ProductId, ItemsPurchasedLast12Months\r\n",
							"    from (select *,\r\n",
							"                row_number() over (partition by UserId order by ItemsPurchasedLast12Months desc) as seqnum\r\n",
							"        from top_purchases\r\n",
							"        ) a\r\n",
							"    where seqnum <= 5 and IsTopProduct == true and IsPreferredProduct = true\r\n",
							"    order by a.UserId"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"top5Products = sqlContext.table(\"top_5_products\")\r\n",
							"\r\n",
							"top5Products.show(100)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('before filter: ', topPreferredProducts.count(), ', after filter: ', top5Products.count())"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"top5ProductsOverall = (top5Products.select(\"ProductId\",\"ItemsPurchasedLast12Months\")\r\n",
							"    .groupBy(\"ProductId\")\r\n",
							"    .agg( sum(\"ItemsPurchasedLast12Months\").alias(\"Total\") )\r\n",
							"    .orderBy( col(\"Total\").desc() )\r\n",
							"    .limit(5))\r\n",
							"\r\n",
							"top5ProductsOverall.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"import uuid\r\n",
							"\r\n",
							"# Generate random GUID\r\n",
							"runId = uuid.uuid4()\r\n",
							"\r\n",
							"# runId is NOT overridden by parameters in this cell!\r\n",
							"top5ProductsOverall.write.parquet('abfss://wwi-02@asadatalake783871.dfs.core.windows.net/top5-products/' + str(runId) + '.parquet')\r\n",
							"\r\n",
							"# A random, local variable\r\n",
							"anyvar = \"tilfeldigtest\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ...however, runId IS overridden by parameters in this cell! So parameter variables are overridden in new cells, \r\n",
							"top5ProductsOverall.write.parquet('abfss://wwi-02@asadatalake783871.dfs.core.windows.net/top5-products/' + str(runId) + '.parquet')\r\n",
							"\r\n",
							"# ... while other variables are kept as they are\r\n",
							"top5ProductsOverall.write.parquet('abfss://wwi-02@asadatalake783871.dfs.core.windows.net/top5-products/' + (anyvar if anyvar else \"fantestikke\") + '.parquet')\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Explore with Spark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fa4a0cdc-013a-4081-81c5-e13752f7a64f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e1b7229d-48f4-49dd-a500-471738fa34b2/resourceGroups/data-engineering-synapse-783871/providers/Microsoft.Synapse/workspaces/asaworkspace783871/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://asaworkspace783871.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Exploring and fixing data with Synapse Spark\r\n",
							"\r\n",
							"In this task, you will use a Synapse Spark notebook to explore a few of the files in the **wwi-02/sale-poc** folder in the data lake. You will also use Python code to fix the issues with the **sale-20170502.csv** file.\r\n",
							"\r\n",
							"1. First, attach this notebook to the **SparkPool01** Spark pool.\r\n",
							"2. In the code cell below, replace **asadatalake*SUFFIX*** `with the name of the primary data lake storage account associated with your Syanpse workspace. Then execute the cell by selecting the **Run cell** button that becomes visible when you select the cell.\r\n",
							"\r\n",
							"> **Note**: The cell may take some time to run because the spark cluster must be started."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"adls_account_name = 'asadatalake783871'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploring files with Spark\r\n",
							"\r\n",
							"1. The first step in exploring data using Synapse Spark is to load a file from the data lake. For this, we'll use the **spark.read.load()** method of the **SparkSession** to load the **sale-20170501.csv** file into a [DataFrame](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes).\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# First, load the file `sale-20170501.csv` file, which we know from our previous exploration to be formatted correctly.\r\n",
							"# Note the use of the `header` and `inferSchema` parameters. Header indicates the first row of the file contains column headers,\r\n",
							"# and `inferSchema` instruct Spark to use data within the file to infer data types.\r\n",
							"df = spark.read.load(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170501.csv', format='csv', header=True, inferSchema=True)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## View the contents of the DataFrame\r\n",
							"\r\n",
							"With the data from the **sale-20170501.csv** file loaded into a data frame, we can now use various methods of a data frame to explore the properties of the data.\r\n",
							"\r\n",
							"1. Let's look at the data as it was imported. Execute the cell below to view and inspect the data in the data frame."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"TransactionId"
									],
									"values": [
										"CustomerId"
									],
									"yLabel": "CustomerId",
									"xLabel": "TransactionId",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"CustomerId\":{\"cdd2ed88-8aae-4295-884a-ac4d40c3c33c\":44,\"e067fc11-e07d-4517-bc93-f7dc4b44f35e\":18}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"2. Like we saw during exploration with the SQL on-demand capabilities of Azure Synapse, Spark allows us to view and query against the data contained within files. \r\n",
							"\r\n",
							"3. Now, use the **printSchema()** method of the data frame to view the results of using the **inferSchema** parameter when creating the data frame. Execute the cell below and observe the output."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Now, print the inferred schema. We will need this information below to help with the missing headers in the May 2, 2017 file.\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"4. The **printSchema** method outputs both field names and data types that are based on the Spark engine's evaluation of the data contained within each field.\r\n",
							"\r\n",
							"    > We can use this information later to help define the schema for the poorly formed **sale-20170502.csv** file. In addition to the field names and data types, we should note the number of features or columns contained in the file. In this case, note that there are 11 fields. That will be used to determine where to split the single row of data.\r\n",
							"\r\n",
							"5. As an example of further exploration we can do, run the cell below to create and display a new data frame that contains an ordered list of distinct Customer and Product Id pairings. We can use these types of functions to find invalid or empty values quickly in targeted fields."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [],
									"values": [
										"ProductId"
									],
									"yLabel": "ProductId",
									"xLabel": "",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"ProductId\":{\"\":189206}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"# Create a new data frame containing a list of distinct CustomerId and ProductId values in descending order of the CustomerId.\r\n",
							"df_distinct_products = df.select('CustomerId', 'ProductId').distinct().orderBy('CustomerId')\r\n",
							"\r\n",
							"# Display the first 100 rows of the resulting data frame.\r\n",
							"display(df_distinct_products.limit(100))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"6. Next, let's attempt to open and explore the **sale-20170502.csv** file using the **load()** method, as we did above."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Next, let's try to read in the May 2, 2017 file using the same `load()` method we used for the first file.\r\n",
							"df = spark.read.load(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502.csv', format='csv')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"7. As we saw in T-SQL, we receive a similar error in Spark that the number of columns processed may have exceeded limit of 20480 columns. To work with the data in this file, we need to use more advanced methods, as you will see in the next section below.\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Handling and fixing poorly formed CSV files\r\n",
							"\r\n",
							"> The steps below provide example code for fixing the poorly-formed CSV file, **sale-20170502.csv** we discovered during exploration of the files in the **wwi-02/sale-poc** folder. This is just one of many ways to handle \"fixing\" a poorly-formed CSV file using Spark.\r\n",
							"\r\n",
							"1. To \"fix\" the bad file, we need to take a programmatic approach, using Python to read in the contents of the file and then parse them to put them into the proper shape.\r\n",
							"\r\n",
							"    > To handle the data being in a single row, we can use the **textFile()** method of our **SparkContext** to read the file as a collection of rows into a resilient distributed dataset (RDD). This allows us to get around the errors around the number of columns because we are essentially getting a single string value stored in a single column.\r\n",
							"\r\n",
							"2. Execute the cell below to load the RDD with data from the file."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import the NumPy library. NumPy is a python library used for working with arrays.\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"# Read the CSV file into a resilient distributed dataset (RDD) as a text file. This will read each row of the file into rows in an RDD.\r\n",
							"rdd = sc.textFile(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502.csv')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"3. With the data now stored in an RDD, we can access the first, and only, populated row in the RDD, and split that into individual fields. We know from our inspection of the file in Notepad++ that it all the fields are separated by a comma (,), so let's start by splitting on that to create an array of field values. Execute the cell below to create a data array."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Since we know there is only one row, grab the first row of the RDD and split in on the field delimiter (comma).\r\n",
							"data = rdd.first().split(',')\r\n",
							"\r\n",
							"field_count = len(data)\r\n",
							"# Print out the count of fields read into the array.\r\n",
							"print(field_count)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"4. By splitting the row on the field delimiter, we created an array of all the individual field values in the file, the count of which you can see above.\n",
							"\n",
							"5. Now, run the cell below to do a quick calculation on the expected number of rows that will be generated by parsing every 11 fields into a single row."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import math\r\n",
							"\r\n",
							"expected_row_count = math.floor(field_count / 11)\r\n",
							"print(f'The expected row count is: {expected_row_count}')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"6. Next, let's create an array to store the data associated with each \"row\".\r\n",
							"\r\n",
							"    > We will set the max_index to the number of columns that are expected in each row. We know from our exploration of other files in the **wwi-02/sale-poc** folder that they contain 11 columns, so that is the value we will set.\r\n",
							"\r\n",
							"7. In addition to setting variables, we will use the cell below to loop through the **data** array and assign every 11 values to a row. By doing this, we are able to \"split\" the data that was once a single row into appropriate rows containing the proper data and columns from the file.\r\n",
							"\r\n",
							"8. Execute the cell below to create an array of rows from the file data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Create an array to store the data associated with each \"row\". Set the max_index \r\n",
							"# to the number of columns that are in each row. This is 11, which we noted above \r\n",
							"# when viewing the schema of the May 1 file.\r\n",
							"row_list = []\r\n",
							"max_index = 11\r\n",
							"\r\n",
							"# Now, we are going to loop through the array of values extracted from the single \r\n",
							"# row of the file and build rows consisting of 11 columns.\r\n",
							"while max_index <= len(data):\r\n",
							"    row = [data[i] for i in np.arange(max_index-11, max_index)]\r\n",
							"    row_list.append(row)\r\n",
							"    max_index += 11\r\n",
							"\r\n",
							"print(\r\n",
							"    f'The row array contains {len(row_list)} rows. The expected number' \r\n",
							"    f' of rows was {expected_row_count}.'\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"row = []\r\n",
							"row_list = []\r\n",
							"\r\n",
							"for i, d in enumerate(data):\r\n",
							"    row.append(d)\r\n",
							"    if i % 11 == 10:\r\n",
							"        row_list.append(row)\r\n",
							"        row = []\r\n",
							"print(\r\n",
							"    f'The row array contains {len(row_list)} rows. The expected number ' \r\n",
							"    f'of rows was {expected_row_count}.'\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"row_list[0]\", row_list[0])\r\n",
							"print(\"row_list[-1]\", row_list[-1])"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"9. The last thing we need to do to be able to work with the file data as rows is to read it into a Spark DataFrame. In the cell below, we use the **createDataFrame()** method to convert the **row_list** array into a data frame, which also adding names for the columns. Column names are based on the schema we observed in the well formatted files in the **wwi-02/sale-poc** directory.\r\n",
							"\r\n",
							"10. Execute the cell below to create a data frame containing row data from the file and then display the first 10 rows."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"TransactionId"
									],
									"values": [
										"TransactionId"
									],
									"yLabel": "TransactionId",
									"xLabel": "TransactionId",
									"aggregation": "COUNT",
									"aggByBackend": false
								},
								"aggData": "{\"TransactionId\":{\"5455a4b4-62bd-401a-b5c6-79ea24f30531\":5,\"a4116581-5aad-416a-b767-aefa516737b1\":5}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"# Finally, we can use the row_list we created above to create a DataFrame. We can \r\n",
							"# add to this a schema parameter, which contains the column names we saw in the \r\n",
							"# schema of the first file.\r\n",
							"df_fixed = spark.createDataFrame(row_list,schema=[\r\n",
							"    'TransactionId', 'CustomerId', 'ProductId', 'Quantity', 'Price', 'TotalAmount',\r\n",
							"    'TransactionDateId', 'ProfitAmount', 'Hour', 'Minute', 'StoreId'\r\n",
							"])\r\n",
							"display(df_fixed.limit(10))"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write the \"fixed\" file into the data lake\r\n",
							"\r\n",
							"1. The last step we will take as part of our exploration and file fixing process is to write the data back into the data lake, so it can be ingested following the same process as the other files in the **wwi-02/sale-poc** folder.\r\n",
							"\r\n",
							"2. Execute the cell below to save the data frame into the data lake a series of files in a folder named **sale-20170502-fixed**.\r\n",
							"\r\n",
							"    > Note: Spark parallelizes workloads across worker nodes, so when saving files, you will notice they are saved as a collection \"part\" files, and not as a single file. While there are some libraries you can use to create a single file, it is helpful to get used to working with files generated via Spark notebooks as they are natively created.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_fixed.write.format('csv').option('header',True).mode('overwrite') \\\r\n",
							"    .option('sep',',').save(\r\n",
							"        f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502-fixed'\r\n",
							"    )"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Inspect the fixed file in the data lake\r\n",
							"\r\n",
							"1. With the fixed file written to the data lake, you can quickly inpsect it to verify the files are now formatted properly. Select the **wwi-02** tab above to view the **sale-poc** folder.\r\n",
							"2. Refresh the folder view (expand the **More** menu if necessary) and then open the **sale-20170502-fixed** folder.\r\n",
							"3. In the **sale-20170502-fixed** folder, right-click the first file whose name begins with **part** and whose extension is **.csv** and select **Preview** from the context menu.\r\n",
							"4. In the **Preview** dialog, verify you see the proper columns and that the data looks valid in each field.\r\n",
							"\r\n",
							"## Wrap-up\r\n",
							"\r\n",
							"Throughout this exercise, you used a Spark notebook to explore data stored within files in the data lake. You used Python code to extract data from a poorly formatted CSV file, assemble the data from that file into proper rows, and then write the \"fixed\" file back out into your data lake.\r\n",
							"\r\n",
							"You can now return to the lab guide to continue with the next section of Lab 2.\r\n",
							""
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 6 - oppg 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d4a4bec5-7d14-4390-8ca1-ac3c2cd32a60"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Exercise 1 - Code-free transformation at scale with Azure Synapse Pipelines\r\n",
							"Tailwind Traders would like code-free options for data engineering tasks. Their motivation is driven by the desire to allow junior-level data engineers who understand the data but do not have a lot of development experience build and maintain data transformation operations. The other driver for this requirement is to reduce fragility caused by complex code with reliance on libraries pinned to specific versions, remove code testing requirements, and improve ease of long-term maintenance.\r\n",
							"\r\n",
							"Their other requirement is to maintain transformed data in a data lake in addition to the dedicated SQL pool. This gives them the flexibility to retain more fields in their data sets than they otherwise store in fact and dimension tables, and doing this allows them to access the data when they have paused the dedicated SQL pool, as a cost optimization.\r\n",
							"\r\n",
							"Given these requirements, you recommend building Mapping Data Flows.\r\n",
							"\r\n",
							"Mapping Data flows are pipeline activities that provide a visual way of specifying how to transform data, through a code-free experience. This feature offers data cleansing, transformation, aggregation, conversion, joins, data copy operations, etc.\r\n",
							"\r\n",
							"Additional benefits\r\n",
							"\r\n",
							"- Cloud scale via Spark execution\r\n",
							"- Guided experience to easily build resilient data flows\r\n",
							"- Flexibility to transform data per user’s comfort\r\n",
							"- Monitor and manage data flows from a single pane of glass"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 6 - oppg 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3e8ea3bc-d88a-4bc4-b1ff-2ff0c5e66354"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "68633f87-6167-4567-a44b-69cf13ccf1fa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Lab 7 - Integrate data from notebooks with Azure Data Factory or Azure Synapse Pipelines\r\n",
							"\r\n",
							"You will learn how to create linked services, and orchestrate data movement and transformation in Azure Synapse Pipelines.\r\n",
							"\r\n",
							"After completing this lab, you will be able to:\r\n",
							"\r\n",
							"* Orchestrate data movement and transformation in Azure Synapse Pipelines"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Exercise 1 - Create mapping data flow and pipeline\r\n",
							"\r\n",
							"In this exercise, you create a mapping data flow that copies user profile data to the data lake, then create a pipeline that orchestrates executing the data flow, and later on, the Spark notebook you create later in this lab."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Exercise 2 - Create Synapse Spark notebook to find top products\r\n",
							"\r\n",
							"Tailwind Traders uses a Mapping Data flow in Synapse Analytics to process, join, and import user profile data. Now they want to find the top 5 products for each user, based on which ones are both preferred and top, and have the most purchases in the past 12 months. Then, they want to calculate the top 5 products overall.\r\n",
							"\r\n",
							"In this exercise, you will create a Synapse Spark notebook to make these calculations."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b369a6be-835d-4526-89dd-fb697eb01d75"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Lab 8 - End-to-end security with Azure Synapse Analytics\r\n",
							"\r\n",
							"In this lab, you will learn how to secure a Synapse Analytics workspace and its supporting infrastructure. You will observe the SQL Active Directory Admin, manage IP firewall rules, manage secrets with Azure Key Vault and access those secrets through a Key Vault linked service and pipeline activities. You will understand how to implement column-level security, row-level security, and dynamic data masking when using dedicated SQL pools.\r\n",
							"\r\n",
							"After completing this lab, you will be able to:\r\n",
							"\r\n",
							"* Secure Azure Synapse Analytics supporting infrastructure\r\n",
							"* Secure the Azure Synapse Analytics workspace and managed services\r\n",
							"* Secure Azure Synapse Analytics workspace data\r\n",
							"\r\n",
							"This lab will guide you through several security-related steps that cover an end-to-end security story for Azure Synapse Analytics. Some key take-aways from this lab are:\r\n",
							"\r\n",
							"1. Leverage Azure Key Vault to store sensitive connection information, such as access keys and passwords for linked services as well as in pipelines.\r\n",
							"\r\n",
							"2. Introspect the data that is contained within the SQL Pools in the context of potential sensitive/confidential data disclosure. Identify the columns representing sensitive data, then secure them by adding column-level security. Determine at the table level what data should be hidden from specific groups of users then define security predicates to apply row level security (filters) on the table. If desired, you also have the option of applying Dynamic Data Masking to mask sensitive data returned in queries on a column by column basis."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Exercise 1 - Securing Azure Synapse Analytics supporting infrastructure\r\n",
							"\r\n",
							"Azure Synapse Analytics (ASA) is a powerful solution that handles security for many of the resources that it creates and manages. In order to run ASA, however, some foundational security measures need to be put in place to ensure the infrastructure that it relies upon is secure. In this exercise, we will walk through securing the supporting infrastructure of ASA."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 9 ex 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8e005692-582d-40c2-8e5a-077760399a90"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e1b7229d-48f4-49dd-a500-471738fa34b2/resourceGroups/data-engineering-synapse-783871/providers/Microsoft.Synapse/workspaces/asaworkspace783871/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://asaworkspace783871.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Lab 9 exercise 1\r\n",
							"\r\n",
							"Ikke gjort denne\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Read from Cosmos DB analytical store into a Spark DataFrame and display 10 rows from the DataFrame\n",
							"# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
							"\n",
							"df = spark.read\\\n",
							"    .format(\"cosmos.olap\")\\\n",
							"    .option(\"spark.synapse.linkedService\", \"asacosmosdb01\")\\\n",
							"    .option(\"spark.cosmos.container\", \"OnlineUserProfile01\")\\\n",
							"    .load()\n",
							"\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"unwanted_cols = {'_attachments','_etag','_rid','_self','_ts','collectionType','id'}\r\n",
							"\r\n",
							"# Remove unwanted columns from the columns collection\r\n",
							"cols = list(set(df.columns) - unwanted_cols)\r\n",
							"\r\n",
							"profiles = df.select(cols)\r\n",
							"\r\n",
							"display(profiles.limit(10)) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"profiles.count()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import udf, explode\r\n",
							"\r\n",
							"preferredProductsFlat=profiles.select('userId',explode('preferredProducts').alias('productId'))\r\n",
							"productReviewsFlat=profiles.select('userId',explode('productReviews').alias('productReviews'))\r\n",
							"display(productReviewsFlat.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(preferredProductsFlat.limit(20))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"productReviews = (productReviewsFlat.select('userId','productReviews.productId','productReviews.reviewText')\r\n",
							"    .orderBy('userId'))\r\n",
							"\r\n",
							"display(productReviews.limit(10))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"preferredProductReviews = (preferredProductsFlat.join(productReviews,\r\n",
							"    (preferredProductsFlat.userId == productReviews.userId) &\r\n",
							"    (preferredProductsFlat.productId == productReviews.productId))\r\n",
							")\r\n",
							"\r\n",
							"display(preferredProductReviews.limit(100))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab 9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c091b963-6a1e-420f-8e93-959bc09fe898"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e1b7229d-48f4-49dd-a500-471738fa34b2/resourceGroups/data-engineering-synapse-783871/providers/Microsoft.Synapse/workspaces/asaworkspace783871/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://asaworkspace783871.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Lab 9 - Support Hybrid Transactional Analytical Processing (HTAP) with Azure Synapse Link\r\n",
							"\r\n",
							"In this lab, you will learn how Azure Synapse Link enables seamless connectivity of an Azure Cosmos DB account to a Synapse workspace. You will learn how to enable and configure Synapse link, then how to query the Azure Cosmos DB analytical store using Apache Spark and SQL Serverless.\r\n",
							"\r\n",
							"After completing this lab, you will be able to:\r\n",
							"\r\n",
							"- Configure Azure Synapse Link with Azure Cosmos DB\r\n",
							"- Query Azure Cosmos DB with Apache Spark for Synapse Analytics\r\n",
							"- Query Azure Cosmos DB with serverless SQL pool for Azure Synapse Analytics"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Exercise 1 - Configuring Azure Synapse Link with Azure Cosmos DB\r\n",
							"\r\n",
							"Tailwind Traders uses Azure Cosmos DB to store user profile data from their eCommerce site. The NoSQL document store provided by the Azure Cosmos DB SQL API provides the familiarity of managing their data using SQL syntax, while being able to read and write the files at a massive, global scale.\r\n",
							"\r\n",
							"While Tailwind Traders is happy with the capabilities and performance of Azure Cosmos DB, they are concerned about the cost of executing a large volume of analytical queries over multiple partitions (cross-partition queries) from their data warehouse. They want to efficiently access all the data without needing to increase the Azure Cosmos DB request units (RUs). They have looked at options for extracting data from their containers to the data lake as it changes, through the Azure Cosmos DB change feed mechanism. The problem with this approach is the extra service and code dependencies and long-term maintenance of the solution. They could perform bulk exports from a Synapse Pipeline, but then they won't have the most up-to-date information at any given moment.\r\n",
							"\r\n",
							"You decide to enable Azure Synapse Link for Cosmos DB and enable the analytical store on their Azure Cosmos DB containers. With this configuration, all transactional data is automatically stored in a fully isolated column store. This store enables large-scale analytics against the operational data in Azure Cosmos DB, without impacting the transactional workloads or incurring resource unit (RU) costs. Azure Synapse Link for Cosmos DB creates a tight integration between Azure Cosmos DB and Azure Synapse Analytics, which enables Tailwind Traders to run near real-time analytics over their operational data with no-ETL and full performance isolation from their transactional workloads.\r\n",
							"\r\n",
							"By combining the distributed scale of Cosmos DB's transactional processing with the built-in analytical store and the computing power of Azure Synapse Analytics, Azure Synapse Link enables a Hybrid Transactional/Analytical Processing (HTAP) architecture for optimizing Tailwind Trader's business processes. This integration eliminates ETL processes, enabling business analysts, data engineers & data scientists to self-serve and run near real-time BI, analytics, and Machine Learning pipelines over operational data."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Exercise 2 - Querying Azure Cosmos DB with Apache Spark for Synapse Analytics\r\n",
							"\r\n",
							"Tailwind Traders wants to use Apache Spark to run analytical queries against the new Azure Cosmos DB container. In this segment, we will use built-in gestures in Synapse Studio to quickly create a Synapse Notebook that loads data from the analytical store of the HTAP-enabled container, without impacting the transactional store.\r\n",
							"\r\n",
							"Tailwind Traders is trying to solve how they can use the list of preferred products identified with each user, coupled with any matching product IDs in their review history, to show a list of all preferred product reviews."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkPool01",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ed392959-053d-42ca-90fb-99ea082fdf5a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e1b7229d-48f4-49dd-a500-471738fa34b2/resourceGroups/data-engineering-synapse-783871/providers/Microsoft.Synapse/workspaces/asaworkspace783871/bigDataPools/SparkPool01",
						"name": "SparkPool01",
						"type": "Spark",
						"endpoint": "https://asaworkspace783871.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool01",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://wwi-02@asadatalake783871.dfs.core.windows.net/sale-small/Year=2019/Quarter=Q4/Month=12/Day=20191231/sale-small-20191231-snappy.parquet', format='parquet')\r\n",
							"display(df.limit(10))\r\n",
							"datalake = 'asadatalake783871'\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.printSchema()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"profitByDateProduct = (df.groupBy(\"TransactionDate\",\"ProductId\")\r\n",
							"    .agg(\r\n",
							"        sum(\"ProfitAmount\").alias(\"(sum)ProfitAmount\"),\r\n",
							"        round(avg(\"Quantity\"), 4).alias(\"(avg)Quantity\"),\r\n",
							"        sum(\"Quantity\").alias(\"(sum)Quantity\"))\r\n",
							"    .orderBy(\"TransactionDate\"))\r\n",
							"display(profitByDateProduct.limit(100))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = (spark.read \\\r\n",
							"        .option('inferSchema', 'true') \\\r\n",
							"        .json('abfss://wwi-02@' + datalake + '.dfs.core.windows.net/online-user-profiles-02/*.json', multiLine=True)\r\n",
							"    )\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# create a view called user_profiles\r\n",
							"df.createOrReplaceTempView(\"user_profiles\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM user_profiles LIMIT 10"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import udf, explode\r\n",
							"\r\n",
							"flat=df.select('visitorId',explode('topProductPurchases').alias('topProductPurchases_flat'))\r\n",
							"flat.show(100)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"topPurchases = (flat.select('visitorId','topProductPurchases_flat.productId','topProductPurchases_flat.itemsPurchasedLast12Months')\r\n",
							"    .orderBy('visitorId'))\r\n",
							"\r\n",
							"topPurchases.show(100)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Let's order by the number of items purchased in the last 12 months\r\n",
							"sortedTopPurchases = topPurchases.orderBy(\"itemsPurchasedLast12Months\")\r\n",
							"\r\n",
							"display(sortedTopPurchases.limit(100))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sortedTopPurchases = (topPurchases\r\n",
							"    .orderBy( col(\"itemsPurchasedLast12Months\").desc() ))\r\n",
							"\r\n",
							"display(sortedTopPurchases.limit(100))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"groupedTopPurchases = (sortedTopPurchases.select(\"visitorId\")\r\n",
							"    .groupBy(\"visitorId\")\r\n",
							"    .agg(count(\"*\").alias(\"total\"))\r\n",
							"    .orderBy(\"visitorId\") )\r\n",
							"\r\n",
							"display(groupedTopPurchases.limit(100))"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"groupedTopPurchases = (sortedTopPurchases.select(\"visitorId\",\"itemsPurchasedLast12Months\")\r\n",
							"    .groupBy(\"visitorId\")\r\n",
							"    .agg(sum(\"itemsPurchasedLast12Months\").alias(\"totalItemsPurchased\"))\r\n",
							"    .orderBy(\"visitorId\") )\r\n",
							"\r\n",
							"display(groupedTopPurchases.limit(100))"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create a temporary view for top purchases so we can load from Scala\r\n",
							"topPurchases.createOrReplaceTempView(\"top_purchases\")"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"// Make sure the name of the dedcated SQL pool (SQLPool01 below) matches the name of your SQL pool.\r\n",
							"val df = spark.sqlContext.sql(\"select * from top_purchases\")\r\n",
							"df.write.synapsesql(\"SQLPool01.wwi.TopPurchases\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfsales = spark.read.load('abfss://wwi-02@' + datalake + '.dfs.core.windows.net/sale-small/Year=2019/Quarter=Q4/Month=12/*/*.parquet', format='parquet')\r\n",
							"display(dfsales.limit(10))"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"// Make sure the name of the SQL pool (SQLPool01 below) matches the name of your SQL pool.\r\n",
							"val df2 = spark.read.synapsesql(\"SQLPool01.wwi.TopPurchases\")\r\n",
							"df2.createTempView(\"top_purchases_sql\")\r\n",
							"\r\n",
							"df2.head(10)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfTopPurchasesFromSql = sqlContext.table(\"top_purchases_sql\")\r\n",
							"\r\n",
							"display(dfTopPurchasesFromSql.limit(10))"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"inner_join = dfsales.join(dfTopPurchasesFromSql,\r\n",
							"    (dfsales.CustomerId == dfTopPurchasesFromSql.visitorId) & (dfsales.ProductId == dfTopPurchasesFromSql.productId))\r\n",
							"\r\n",
							"inner_join_agg = (inner_join.select(\"CustomerId\",\"TotalAmount\",\"Quantity\",\"itemsPurchasedLast12Months\",\"top_purchases_sql.productId\")\r\n",
							"    .groupBy([\"CustomerId\",\"top_purchases_sql.productId\"])\r\n",
							"    .agg(\r\n",
							"        sum(\"TotalAmount\").alias(\"TotalAmountDecember\"),\r\n",
							"        sum(\"Quantity\").alias(\"TotalQuantityDecember\"),\r\n",
							"        sum(\"itemsPurchasedLast12Months\").alias(\"TotalItemsPurchasedLast12Months\"))\r\n",
							"    .orderBy(\"CustomerId\") )\r\n",
							"\r\n",
							"display(inner_join_agg.limit(100))"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lab 6 oppg 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "29967688-aa87-40e7-88a1-822486db268c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Exercise 2 - Create Mapping Data Flow for top product purchases\r\n",
							"Tailwind Traders needs to combine top product purchases imported as JSON files from their eCommerce system with user preferred products from profile data stored as JSON documents in Azure Cosmos DB. They want to store the combined data in a dedicated SQL pool as well as their data lake for further analysis and reporting.\r\n",
							"\r\n",
							"To do this, you will build a mapping data flow that performs the following tasks:\r\n",
							"\r\n",
							"- Adds two ADLS Gen2 data sources for the JSON data\r\n",
							"- Flattens the hierarchical structure of both sets of files\r\n",
							"- Performs data transformations and type conversions\r\n",
							"- Joins both data sources\r\n",
							"- Creates new fields on the joined data set based on conditional logic\r\n",
							"- Filters null records for required fields\r\n",
							"- Writes to the dedicated SQL pool\r\n",
							"- Simultaneously writes to the data lake\r\n",
							""
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkPool01')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLPool01')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}